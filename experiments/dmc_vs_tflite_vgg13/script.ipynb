{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# VGG-13 TFLite vs. DMC Comparison Guide\n",
    "\n",
    "This notebook reproduces the VGG-13 experiments comparing TensorFlow Lite (TFLite) and Deep Microcompression (DMC) performanc (Table 3) from the \"Deep Microcompression\" paper.\n",
    "\n",
    "## Required File Structure\n",
    "\n",
    "This script assumes it is located within the original project's directory structure under the experiments directory. The development module must be accessible two levels up.\n",
    "\n",
    "## Experiment Overview\n",
    "\n",
    "The experiment compares accuracy and model size across three quantization schemes:\n",
    "1. **Float32 (Baseline):** No quantization.\n",
    "2. **Dynamic Quantization:** Weights are quantized, activations dynamically quantized at runtime.\n",
    "3. **Static Quantization (Int8):** Weights and activations are quantized using calibration data.\n",
    "\n",
    "## Methodology\n",
    "\n",
    "1. **Source of Truth:** Uses a pre-trained VGG-13 (Batch Norm) model from PyTorch Hub (`cifar100_vgg13_bn`).\n",
    "2. **Weight Transfer:** Copies weights from the PyTorch model to an equivalent TensorFlow/Keras model to ensure an exact baseline match.\n",
    "3. **DMC Conversion:** Converts the PyTorch model to a DMC `Sequential` model.\n",
    "4. **TFLite Conversion:** Converts the Keras model to TFLite flatbuffers using standard optimization flags.\n",
    "5. **Evaluation:** Both frameworks evaluate on the same CIFAR-100 test set. PyTorch is forced to CPU to match the TFLite execution environment.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing the necessary libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "import os\n",
    "os.environ[\"TF_CPP_MIN_LOG_LEVEL\"] = \"2\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"\"  # Force TensorFlow to CPU\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/matthias/Documents/EmbeddedAI/deep-microcompression/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "2026-01-31 01:12:19.054505: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1769818339.070235   37655 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1769818339.075162   37655 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1769818339.086714   37655 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1769818339.086734   37655 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1769818339.086736   37655 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1769818339.086737   37655 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[31mAttributeError\u001b[39m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[31mAttributeError\u001b[39m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[31mAttributeError\u001b[39m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[31mAttributeError\u001b[39m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[31mAttributeError\u001b[39m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "import os\n",
    "import random\n",
    "\n",
    "try:\n",
    "    import numpy as np\n",
    "    from tqdm.auto import tqdm\n",
    "    \n",
    "    import torch\n",
    "    from torch import nn\n",
    "    from torch.utils import data\n",
    "    from torchvision import datasets, transforms\n",
    "\n",
    "    import tensorflow as tf\n",
    "\n",
    "except ImportError:\n",
    "    %pip install torch torchvision tqdm\n",
    "\n",
    "    import numpy as np\n",
    "    from tqdm.auto import tqdm\n",
    "\n",
    "    import torch\n",
    "    from torch import nn\n",
    "    from torch.utils import data\n",
    "    from torchvision import datasets, transforms\n",
    "    from tqdm.auto import tqdm\n",
    "\n",
    "    import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This assumes the script is in 'project_root/experiments/vgg_comparison'\n",
    "sys.path.append(\"../../\")\n",
    "\n",
    "try:\n",
    "    from development.models.utils import convert_from_sequential_torch_to_dmc\n",
    "    from development import (\n",
    "        QuantizationScheme,\n",
    "        QuantizationGranularity\n",
    "    )\n",
    "except ImportError:\n",
    "    print(\"Error: Could not import the 'development' module.\")\n",
    "    print(\"Please ensure this script is run from the correct directory\")\n",
    "    print(\"and the 'development' module is in the project root ('../../').\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# --- Constants ---\n",
    "DEVICE = \"cpu\"  # Force PyTorch to CPU for fair comparison with TFLite CPU execution\n",
    "print(f\"Using device: {DEVICE}\")\n",
    "\n",
    "INPUT_SHAPE_TORCH = (1, 3, 32, 32)\n",
    "INPUT_SHAPE_TF = (32, 32, 3)\n",
    "INPUT_SHAPE = (3, 32, 32)\n",
    "DATASET_DIR = \"../../Datasets/\"\n",
    "\n",
    "PROJECT_BASE_DIR = \"../Arduino Nano 33 BLE\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "LUCKY_NUMBER = 25\n",
    "\n",
    "# Set random seeds for reproducibility\n",
    "torch.manual_seed(LUCKY_NUMBER)\n",
    "tf.random.set_seed(LUCKY_NUMBER)\n",
    "np.random.seed(LUCKY_NUMBER)\n",
    "random.seed(LUCKY_NUMBER)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading CIFAR-100 Dataset\n",
    "We load the dataset with the specific normalization statistics required by the pre-trained model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_loaders():\n",
    "    \"\"\"Loads CIFAR-100 data for both PyTorch and TF evaluation.\"\"\"\n",
    "    print(\"Loading CIFAR-100 dataset...\")\n",
    "    \n",
    "    # Normalization must match the pre-trained model's requirements\n",
    "    data_transform = transforms.Compose([\n",
    "        transforms.Resize((32, 32)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(\n",
    "            mean=(0.5071, 0.4867, 0.4408),\n",
    "            std=(0.2675, 0.2565, 0.2761)\n",
    "        )\n",
    "    ])\n",
    "\n",
    "    cifar100_train_dataset = datasets.CIFAR100(DATASET_DIR, train=True, download=True, transform=data_transform)\n",
    "    cifar100_test_dataset = datasets.CIFAR100(DATASET_DIR, train=False, download=True, transform=data_transform)\n",
    "    \n",
    "    cifar100_train_loader = data.DataLoader(cifar100_train_dataset, batch_size=256, shuffle=True, num_workers=os.cpu_count())\n",
    "    cifar100_test_loader = data.DataLoader(cifar100_test_dataset, batch_size=256, shuffle=False, num_workers=os.cpu_count())\n",
    "    \n",
    "    return cifar100_train_loader, cifar100_test_loader, cifar100_train_dataset, cifar100_test_dataset\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Defining the TensorFlow Model & Weight Transfer\n",
    "To compare against TFLite, we must first construct an equivalent Keras model and copy the weights from the PyTorch source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_tf_vgg13_bn_equivalent():\n",
    "    \"\"\"Creates a Keras Sequential model that mirrors the PyTorch VGG13_BN architecture.\"\"\"\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.InputLayer(input_shape=INPUT_SHAPE_TF),\n",
    "        tf.keras.layers.Conv2D(64, 3, padding='same', name='conv_0'),\n",
    "        tf.keras.layers.BatchNormalization(name='bn_1', epsilon=1e-5),\n",
    "        tf.keras.layers.ReLU(name='relu_2'),\n",
    "        tf.keras.layers.Conv2D(64, 3, padding='same', name='conv_3'),\n",
    "        tf.keras.layers.BatchNormalization(name='bn_4', epsilon=1e-5),\n",
    "        tf.keras.layers.ReLU(name='relu_5'),\n",
    "        tf.keras.layers.MaxPooling2D(2, strides=2, name='pool_6'),\n",
    "        tf.keras.layers.Conv2D(128, 3, padding='same', name='conv_7'),\n",
    "        tf.keras.layers.BatchNormalization(name='bn_8', epsilon=1e-5),\n",
    "        tf.keras.layers.ReLU(name='relu_9'),\n",
    "        tf.keras.layers.Conv2D(128, 3, padding='same', name='conv_10'),\n",
    "        tf.keras.layers.BatchNormalization(name='bn_11', epsilon=1e-5),\n",
    "        tf.keras.layers.ReLU(name='relu_12'),\n",
    "        tf.keras.layers.MaxPooling2D(2, strides=2, name='pool_13'),\n",
    "        tf.keras.layers.Conv2D(256, 3, padding='same', name='conv_14'),\n",
    "        tf.keras.layers.BatchNormalization(name='bn_15', epsilon=1e-5),\n",
    "        tf.keras.layers.ReLU(name='relu_16'),\n",
    "        tf.keras.layers.Conv2D(256, 3, padding='same', name='conv_17'),\n",
    "        tf.keras.layers.BatchNormalization(name='bn_18', epsilon=1e-5),\n",
    "        tf.keras.layers.ReLU(name='relu_19'),\n",
    "        tf.keras.layers.MaxPooling2D(2, strides=2, name='pool_20'),\n",
    "        tf.keras.layers.Conv2D(512, 3, padding='same', name='conv_21'),\n",
    "        tf.keras.layers.BatchNormalization(name='bn_22', epsilon=1e-5),\n",
    "        tf.keras.layers.ReLU(name='relu_23'),\n",
    "        tf.keras.layers.Conv2D(512, 3, padding='same', name='conv_24'),\n",
    "        tf.keras.layers.BatchNormalization(name='bn_25', epsilon=1e-5),\n",
    "        tf.keras.layers.ReLU(name='relu_26'),\n",
    "        tf.keras.layers.MaxPooling2D(2, strides=2, name='pool_27'),\n",
    "        tf.keras.layers.Conv2D(512, 3, padding='same', name='conv_28'),\n",
    "        tf.keras.layers.BatchNormalization(name='bn_29', epsilon=1e-5),\n",
    "        tf.keras.layers.ReLU(name='relu_30'),\n",
    "        tf.keras.layers.Conv2D(512, 3, padding='same', name='conv_31'),\n",
    "        tf.keras.layers.BatchNormalization(name='bn_32', epsilon=1e-5),\n",
    "        tf.keras.layers.ReLU(name='relu_33'),\n",
    "        tf.keras.layers.MaxPooling2D(2, strides=2, name='pool_34'),\n",
    "        tf.keras.layers.Flatten(name='flat_35'),\n",
    "        tf.keras.layers.Dense(512, name='fc_36'),\n",
    "        tf.keras.layers.ReLU(name='relu_37'),\n",
    "        tf.keras.layers.Dropout(0.5, name='drop_38'),\n",
    "        tf.keras.layers.Dense(512, name='fc_39'),\n",
    "        tf.keras.layers.ReLU(name='relu_40'),\n",
    "        tf.keras.layers.Dropout(0.5, name='drop_41'),\n",
    "        tf.keras.layers.Dense(100, name='fc_42')\n",
    "    ], name=\"Custom_VGG13_BN\")\n",
    "\n",
    "def copy_torch_to_tf(pt_state_dict, tf_model):\n",
    "    \"\"\"Copies weights from a PyTorch state_dict to the Keras model.\"\"\"\n",
    "    print(\"Copying weights from PyTorch to Keras...\")\n",
    "    # Mapping of PyTorch Layer Index to Keras Layer Name\n",
    "    conv_layers = [(0, 'conv_0'), (3, 'conv_3'), (7, 'conv_7'), (10, 'conv_10'),\n",
    "                   (14, 'conv_14'), (17, 'conv_17'), (21, 'conv_21'), (24, 'conv_24'),\n",
    "                   (28, 'conv_28'), (31, 'conv_31')]\n",
    "    bn_layers = [(1, 'bn_1'), (4, 'bn_4'), (8, 'bn_8'), (11, 'bn_11'),\n",
    "                 (15, 'bn_15'), (18, 'bn_18'), (22, 'bn_22'), (25, 'bn_25'),\n",
    "                 (29, 'bn_29'), (32, 'bn_32')]\n",
    "    fc_layers = [(36, 'fc_36'), (39, 'fc_39'), (42, 'fc_42')]\n",
    "\n",
    "    # Copy Convolution Weights\n",
    "    for pt_idx, tf_name in conv_layers:\n",
    "        tf_layer = tf_model.get_layer(tf_name)\n",
    "        pt_weight = pt_state_dict[f'{pt_idx}.weight'].detach().numpy()\n",
    "        pt_bias = pt_state_dict[f'{pt_idx}.bias'].detach().numpy()\n",
    "        tf_weight = np.transpose(pt_weight, (2, 3, 1, 0)) # PT (out, in, H, W) -> TF (H, W, in, out)\n",
    "        tf_layer.set_weights([tf_weight, pt_bias])\n",
    "\n",
    "    # Copy BatchNorm Weights\n",
    "    for pt_idx, tf_name in bn_layers:\n",
    "        tf_layer = tf_model.get_layer(tf_name)\n",
    "        gamma = pt_state_dict[f'{pt_idx}.weight'].detach().numpy()\n",
    "        beta = pt_state_dict[f'{pt_idx}.bias'].detach().numpy()\n",
    "        moving_mean = pt_state_dict[f'{pt_idx}.running_mean'].detach().numpy()\n",
    "        moving_variance = pt_state_dict[f'{pt_idx}.running_var'].detach().numpy()\n",
    "        tf_layer.set_weights([gamma, beta, moving_mean, moving_variance])\n",
    "\n",
    "    # Copy Linear Weights\n",
    "    for pt_idx, tf_name in fc_layers:\n",
    "        tf_layer = tf_model.get_layer(tf_name)\n",
    "        pt_weight = pt_state_dict[f'{pt_idx}.weight'].detach().numpy()\n",
    "        pt_bias = pt_state_dict[f'{pt_idx}.bias'].detach().numpy()\n",
    "        tf_weight = np.transpose(pt_weight, (1, 0)) # PT (out, in) -> TF (in, out)\n",
    "        tf_layer.set_weights([tf_weight, pt_bias])\n",
    "    \n",
    "    print(\"Weight copy complete.\")\n",
    "    return tf_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_tf_to_tflite(tf_model, scheme=QuantizationScheme.NONE, train_loader=None):\n",
    "    \"\"\"Converts Keras model to TFLite flatbuffer.\"\"\"\n",
    "    converter = tf.lite.TFLiteConverter.from_keras_model(tf_model)\n",
    "    \n",
    "    if scheme == QuantizationScheme.DYNAMIC:\n",
    "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "    \n",
    "    if scheme == QuantizationScheme.STATIC:\n",
    "        def representative_dataset():\n",
    "            # Use 100 batches from the PyTorch train loader\n",
    "            for i, (batch_images, _) in enumerate(train_loader):\n",
    "                if i >= 100: break\n",
    "                # Permute (B, C, H, W) to TF-style (B, H, W, C)\n",
    "                yield [batch_images.permute(0, 2, 3, 1).numpy().astype(np.float32)]\n",
    "                \n",
    "        converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "        converter.representative_dataset = representative_dataset\n",
    "        converter.target_spec.supported_ops = [tf.lite.OpsSet.TFLITE_BUILTINS_INT8]\n",
    "        converter.inference_input_type = tf.int8\n",
    "        converter.inference_output_type = tf.int8\n",
    "\n",
    "    return converter.convert()\n",
    "\n",
    "\n",
    "def save_tflite_to_board(\n",
    "    tflite_model: bytes,\n",
    "    project_base_dir: str,\n",
    "    var_name: str = \"vgg13\"\n",
    "):\n",
    "    \n",
    "    output_path = os.path.join(project_base_dir, f\"include/{var_name}_tflite.h\")\n",
    "    hex_array = \",\\n  \".join(\n",
    "        f\"0x{b:02x}\" for b in tflite_model\n",
    "    )\n",
    "\n",
    "    c_code = f\"\"\"\\\n",
    "#include <cstdint>\n",
    "\n",
    "alignas(16) const unsigned char {var_name}[] = {{\n",
    "  {hex_array}\n",
    "}};\n",
    "\n",
    "const unsigned int {var_name}_len = {len(tflite_model)};\n",
    "\"\"\"\n",
    "\n",
    "    with open(output_path, \"w\") as f:\n",
    "        f.write(c_code)\n",
    "\n",
    "\n",
    "def save_dmc_to_board(model, project_base_dir, for_arduino=False):\n",
    "    src_dir = os.path.join(project_base_dir, \"src\")\n",
    "    include_dir = os.path.join(project_base_dir, \"include\")\n",
    "    test_image = torch.rand(INPUT_SHAPE, device=DEVICE)\n",
    "\n",
    "    model = model.fuse(device=DEVICE)\n",
    "    model.convert_to_c(\n",
    "        INPUT_SHAPE, \"vgg13_dmc\", \n",
    "        src_dir, include_dir, \n",
    "        for_arduino=for_arduino,\n",
    "        test_input=test_image\n",
    "    )\n",
    "    print(f\"Model has been successfully load to {project_base_dir}.\")\n",
    "\n",
    "    model.eval()\n",
    "    if model.is_quantized and hasattr(model, \"output_quantize\"):\n",
    "        print(f\"The expected output is {model.output_quantize.apply(model(test_image.unsqueeze(0)))}\")\n",
    "    else:\n",
    "        print(f\"The expected output is {model(test_image.unsqueeze(0))}\")\n",
    "\n",
    "\n",
    "def get_tflite_model_accuracy(tflite_model, test_dataset, scheme=QuantizationScheme.NONE):\n",
    "    \"\"\"Evaluates a TFLite flatbuffer model using the PyTorch test dataset.\"\"\"\n",
    "    interpreter = tf.lite.Interpreter(model_content=tflite_model)\n",
    "    interpreter.allocate_tensors()\n",
    "    input_details = interpreter.get_input_details()[0]\n",
    "    output_details = interpreter.get_output_details()[0]\n",
    "\n",
    "    tflite_predicted = []\n",
    "    actual_label = []\n",
    "\n",
    "    for image, label in tqdm(test_dataset, desc=f\"Evaluating TFLite ({scheme.name})\"):\n",
    "        image_np = image.unsqueeze(0).permute(0, 2, 3, 1).numpy() # (1, H, W, C)\n",
    "        \n",
    "        if scheme == QuantizationScheme.STATIC:\n",
    "            scale, zero_point = input_details[\"quantization\"]\n",
    "            image_np = ((image_np / scale) + zero_point).astype(np.int8)\n",
    "        \n",
    "        interpreter.set_tensor(input_details[\"index\"], image_np)\n",
    "        interpreter.invoke()\n",
    "        output_data = interpreter.get_tensor(output_details[\"index\"])\n",
    "        \n",
    "        tflite_predicted.append(np.argmax(output_data))\n",
    "        actual_label.append(label)\n",
    "\n",
    "    tflite_predicted = np.array(tflite_predicted)\n",
    "    actual_label = np.array(actual_label)\n",
    "    return (tflite_predicted == actual_label).mean()\n",
    "\n",
    "\n",
    "def top1_acc_fun(y_pred, y_true):\n",
    "    return (y_pred.argmax(dim=1) == y_true).to(torch.float).mean().item()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading and Preparing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading CIFAR-100 dataset...\n",
      "Loading pre-trained VGG-13 BN from PyTorch Hub...\n",
      "\n",
      "--- Creating TF/Keras Baseline ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2026-01-31 01:12:23.622031: E external/local_xla/xla/stream_executor/cuda/cuda_platform.cc:51] failed call to cuInit: INTERNAL: CUDA error: Failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Copying weights from PyTorch to Keras...\n",
      "Weight copy complete.\n",
      "\n",
      "--- Creating DMC Baseline ---\n"
     ]
    }
   ],
   "source": [
    "# --- Initialization ---\n",
    "results = []\n",
    "\n",
    "# 1. Load Data\n",
    "train_loader, test_loader, train_dataset, test_dataset = get_data_loaders()\n",
    "\n",
    "# 2. Load PyTorch Baseline (The Source of Truth)\n",
    "print(\"Loading pre-trained VGG-13 BN from PyTorch Hub...\")\n",
    "pt_vgg13_hub = torch.hub.load(\"chenyaofo/pytorch-cifar-models\", \"cifar100_vgg13_bn\", pretrained=True, verbose=False)\n",
    "# Flatten the nested structure for easier conversion\n",
    "pt_vgg13_full = (pt_vgg13_hub.features + nn.Sequential(nn.Flatten()) + pt_vgg13_hub.classifier).eval()\n",
    "pt_state_dict = pt_vgg13_full.state_dict()\n",
    "\n",
    "# 3. Create Equivalent Models\n",
    "print(\"\\n--- Creating TF/Keras Baseline ---\")\n",
    "tf_model = create_tf_vgg13_bn_equivalent()\n",
    "tf_model = copy_torch_to_tf(pt_state_dict, tf_model)\n",
    "\n",
    "print(\"\\n--- Creating DMC Baseline ---\")\n",
    "dmc_base_model = convert_from_sequential_torch_to_dmc(pt_vgg13_full).to(DEVICE)\n",
    "dmc_metrics = {\"top1acc\": top1_acc_fun}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NO QUANTIZATION (Float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- STAGE 2: Running Float32 (No Quantization) Comparison ---\n",
      "INFO:tensorflow:Assets written to: /tmp/tmpyst8s54e/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpyst8s54e/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at '/tmp/tmpyst8s54e'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 32, 32, 3), dtype=tf.float32, name='keras_tensor')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 100), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  127017888226128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888227280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888229200: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888229584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888226512: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888228816: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888228624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888229968: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888230352: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888230544: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888229392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888226896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888229008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888227856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888231888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888232080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888230736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888231312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888229776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888231696: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888232848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888233808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888233616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888233040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888233424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888232656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888231504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888234960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888234768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888232272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888234576: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888234192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888235152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888234384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888235344: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888234000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888233232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888228048: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017887647120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017887647312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888220176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017887646160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017887645776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017887646544: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017887646736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017887648464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017887648272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017887646928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017887648080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017887647504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017887646352: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017887649616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017887649424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017887647696: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017887649232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017887648656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017887645968: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017887650768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017887650576: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017887648848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017887650384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017887649808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017887651536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017887650960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017887651920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017887651728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "W0000 00:00:1769818345.445146   37655 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
      "W0000 00:00:1769818345.445164   37655 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n",
      "I0000 00:00:1769818345.474200   37655 mlir_graph_optimization_pass.cc:425] MLIR V1 optimization pass is not enabled\n",
      "INFO: Created TensorFlow Lite XNNPACK delegate for CPU.\n",
      "Evaluating TFLite (NONE): 100%|██████████| 10000/10000 [01:34<00:00, 106.09it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFLite Float: 74.63% | 39936240 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DMC Float:    74.63% | 39949968 bytes\n"
     ]
    }
   ],
   "source": [
    "# --- STAGE 2: NO QUANTIZATION (Float32) ---\n",
    "print(\"\\n--- STAGE 2: Running Float32 (No Quantization) Comparison ---\")\n",
    "\n",
    "# TFLite (Float)\n",
    "tflite_float_model = convert_tf_to_tflite(tf_model, QuantizationScheme.NONE)\n",
    "tflite_float_acc = get_tflite_model_accuracy(tflite_float_model, test_dataset, QuantizationScheme.NONE)\n",
    "tflite_float_size = len(tflite_float_model)\n",
    "results.append((\"TFLite (Float32)\", tflite_float_acc, tflite_float_size))\n",
    "print(f\"TFLite Float: {tflite_float_acc*100:.2f}% | {tflite_float_size} bytes\")\n",
    "\n",
    "# DMC (Float)\n",
    "dmc_float_model = dmc_base_model.init_compress({\n",
    "    \"quantize\": {\"scheme\": QuantizationScheme.NONE, \"activation_bitwidth\": None, \"parameter_bitwidth\": None, \"granularity\": None}\n",
    "    }, INPUT_SHAPE_TORCH)\n",
    "dmc_float_eval = dmc_float_model.evaluate(test_loader, dmc_metrics, device=DEVICE)\n",
    "dmc_float_size = dmc_float_model.get_size_in_bits() // 8\n",
    "results.append((\"DMC (Float32)\", dmc_float_eval['top1acc'], dmc_float_size))\n",
    "print(f\"DMC Float:    {dmc_float_eval['top1acc']*100:.2f}% | {dmc_float_size} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### DYNAMIC QUANTIZATION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- STAGE 3: Running Dynamic Quantization Comparison ---\n",
      "INFO:tensorflow:Assets written to: /tmp/tmp_tznea6t/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmp_tznea6t/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at '/tmp/tmp_tznea6t'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 32, 32, 3), dtype=tf.float32, name='keras_tensor')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 100), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  127017888226128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888227280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888229200: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888229584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888226512: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888228816: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888228624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888229968: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888230352: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888230544: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888229392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888226896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888229008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888227856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888231888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888232080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888230736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888231312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888229776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888231696: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888232848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888233808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888233616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888233040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888233424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888232656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888231504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888234960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888234768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888232272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888234576: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888234192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888235152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888234384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888235344: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888234000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888233232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888228048: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017887647120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017887647312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888220176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017887646160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017887645776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017887646544: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017887646736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017887648464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017887648272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017887646928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017887648080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017887647504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017887646352: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017887649616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017887649424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017887647696: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017887649232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017887648656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017887645968: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017887650768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017887650576: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017887648848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017887650384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017887649808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017887651536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017887650960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017887651920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017887651728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1769818485.302489   37655 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
      "W0000 00:00:1769818485.302510   37655 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n",
      "Evaluating TFLite (DYNAMIC): 100%|██████████| 10000/10000 [00:26<00:00, 381.58it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFLite Dynamic: 74.62% | 10052520 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DMC Dynamic:    74.32% | 10017412 bytes\n"
     ]
    }
   ],
   "source": [
    "# --- STAGE 3: DYNAMIC QUANTIZATION ---\n",
    "print(\"\\n--- STAGE 3: Running Dynamic Quantization Comparison ---\")\n",
    "\n",
    "# TFLite (Dynamic)\n",
    "tflite_dyn_model = convert_tf_to_tflite(tf_model, QuantizationScheme.DYNAMIC)\n",
    "tflite_dyn_acc = get_tflite_model_accuracy(tflite_dyn_model, test_dataset, QuantizationScheme.DYNAMIC)\n",
    "tflite_dyn_size = len(tflite_dyn_model)\n",
    "results.append((\"TFLite (Dynamic)\", tflite_dyn_acc, tflite_dyn_size))\n",
    "print(f\"TFLite Dynamic: {tflite_dyn_acc*100:.2f}% | {tflite_dyn_size} bytes\")\n",
    "\n",
    "# DMC (Dynamic)\n",
    "dmc_dyn_model = dmc_base_model.init_compress({\n",
    "    \"quantize\": {\"scheme\": QuantizationScheme.DYNAMIC, \"activation_bitwidth\": None, \"parameter_bitwidth\": 8, \"granularity\": QuantizationGranularity.PER_TENSOR}\n",
    "}, INPUT_SHAPE_TORCH)\n",
    "dmc_dyn_eval = dmc_dyn_model.evaluate(test_loader, dmc_metrics, device=DEVICE)\n",
    "dmc_dyn_size = dmc_dyn_model.get_size_in_bits() // 8\n",
    "results.append((\"DMC (Dynamic)\", dmc_dyn_eval['top1acc'], dmc_dyn_size))\n",
    "print(f\"DMC Dynamic:    {dmc_dyn_eval['top1acc']*100:.2f}% | {dmc_dyn_size} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### STAGE 3: STATIC QUANTIZATION \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- STAGE 4: Running Static Quantization (INT8) Comparison ---\n",
      "INFO:tensorflow:Assets written to: /tmp/tmpnw48g40o/assets\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Assets written to: /tmp/tmpnw48g40o/assets\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved artifact at '/tmp/tmpnw48g40o'. The following endpoints are available:\n",
      "\n",
      "* Endpoint 'serve'\n",
      "  args_0 (POSITIONAL_ONLY): TensorSpec(shape=(None, 32, 32, 3), dtype=tf.float32, name='keras_tensor')\n",
      "Output Type:\n",
      "  TensorSpec(shape=(None, 100), dtype=tf.float32, name=None)\n",
      "Captures:\n",
      "  127017888226128: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888227280: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888229200: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888229584: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888226512: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888228816: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888228624: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888229968: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888230352: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888230544: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888229392: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888226896: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888229008: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888227856: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888231888: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888232080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888230736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888231312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888229776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888231696: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888232848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888233808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888233616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888233040: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888233424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888232656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888231504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888234960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888234768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888232272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888234576: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888234192: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888235152: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888234384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888235344: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888234000: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888233232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888228048: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017887647120: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017887647312: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017888220176: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017887646160: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017887645776: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017887646544: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017887646736: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017887648464: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017887648272: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017887646928: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017887648080: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017887647504: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017887646352: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017887649616: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017887649424: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017887647696: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017887649232: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017887648656: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017887645968: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017887650768: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017887650576: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017887648848: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017887650384: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017887649808: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017887651536: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017887650960: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017887651920: TensorSpec(shape=(), dtype=tf.resource, name=None)\n",
      "  127017887651728: TensorSpec(shape=(), dtype=tf.resource, name=None)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "W0000 00:00:1769818556.278901   37655 tf_tfl_flatbuffer_helpers.cc:365] Ignored output_format.\n",
      "W0000 00:00:1769818556.278919   37655 tf_tfl_flatbuffer_helpers.cc:368] Ignored drop_control_dependency.\n",
      "fully_quantize: 0, inference_type: 6, input_inference_type: INT8, output_inference_type: INT8\n",
      "Evaluating TFLite (STATIC): 100%|██████████| 10000/10000 [00:23<00:00, 425.77it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFLite Static: 74.44% | 10102360 bytes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "                                                           \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DMC Static:    74.51% | 10033732 bytes\n"
     ]
    }
   ],
   "source": [
    "# --- STAGE 4: STATIC QUANTIZATION ---\n",
    "print(\"\\n--- STAGE 4: Running Static Quantization (INT8) Comparison ---\")\n",
    "\n",
    "# TFLite (Static)\n",
    "tflite_static_model = convert_tf_to_tflite(tf_model, QuantizationScheme.STATIC, train_loader)\n",
    "tflite_static_acc = get_tflite_model_accuracy(tflite_static_model, test_dataset, QuantizationScheme.STATIC)\n",
    "tflite_static_size = len(tflite_static_model)\n",
    "results.append((\"TFLite (Static)\", tflite_static_acc, tflite_static_size))\n",
    "print(f\"TFLite Static: {tflite_static_acc*100:.2f}% | {tflite_static_size} bytes\")\n",
    "\n",
    "# DMC (Static)\n",
    "calib_data_torch = next(iter(train_loader))[0].to(DEVICE)\n",
    "dmc_static_model = dmc_base_model.init_compress({\n",
    "    \"quantize\": {\"scheme\": QuantizationScheme.STATIC, \"activation_bitwidth\": 8, \"parameter_bitwidth\": 8, \"granularity\": QuantizationGranularity.PER_CHANNEL}\n",
    "}, INPUT_SHAPE_TORCH, calibration_data=calib_data_torch)\n",
    "dmc_static_eval = dmc_static_model.evaluate(test_loader, dmc_metrics, device=DEVICE)\n",
    "dmc_static_size = dmc_static_model.get_size_in_bits() // 8\n",
    "results.append((\"DMC (Static)\", dmc_static_eval['top1acc'], dmc_static_size))\n",
    "print(f\"DMC Static:    {dmc_static_eval['top1acc']*100:.2f}% | {dmc_static_size} bytes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "--- REPRODUCTION FINISHED: VGG-13 TFLITE vs. DMC ---\n",
      "============================================================\n",
      "       Method        |  Top-1 Acc (%)  |    Size (MB)   \n",
      "------------------------------------------------------------\n",
      "  TFLite (Float32)   |      74.63      |   38.09   \n",
      "   DMC (Float32)     |      74.63      |   38.10   \n",
      "  TFLite (Dynamic)   |      74.62      |    9.59   \n",
      "   DMC (Dynamic)     |      74.32      |    9.55   \n",
      "  TFLite (Static)    |      74.44      |    9.63   \n",
      "    DMC (Static)     |      74.51      |    9.57   \n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "# --- Print Final Summary Table ---\n",
    "print(\"\\n\\n--- REPRODUCTION FINISHED: VGG-13 TFLITE vs. DMC ---\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"{'Method':^20} | {'Top-1 Acc (%)':^15} | {'Size (MB)':^15}\")\n",
    "print(\"-\" * 60)\n",
    "for name, acc, size in results:\n",
    "    print(f\"{name:^20} | {acc * 100:^15.2f} | {size/(2**20):^10.2f}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "NotImplementedError",
     "evalue": "This is not implement because it should have been fused before deployment.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNotImplementedError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[16]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m save_tflite_to_board(tflite_float_model, PROJECT_BASE_DIR)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43msave_dmc_to_board\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdmc_float_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mPROJECT_BASE_DIR\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 56\u001b[39m, in \u001b[36msave_dmc_to_board\u001b[39m\u001b[34m(model, project_base_dir, for_arduino)\u001b[39m\n\u001b[32m     53\u001b[39m test_image = torch.rand(INPUT_SHAPE, device=DEVICE)\n\u001b[32m     55\u001b[39m model = model.fuse(device=DEVICE)\n\u001b[32m---> \u001b[39m\u001b[32m56\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconvert_to_c\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m    \u001b[49m\u001b[43mINPUT_SHAPE\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mvgg13_dmc\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m    \u001b[49m\u001b[43msrc_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minclude_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[32m     59\u001b[39m \u001b[43m    \u001b[49m\u001b[43mfor_arduino\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfor_arduino\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     60\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtest_input\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtest_image\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     62\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mModel has been successfully load to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mproject_base_dir\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     64\u001b[39m model.eval()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/EmbeddedAI/deep-microcompression/.venv/lib/python3.12/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/EmbeddedAI/deep-microcompression/experiments/reproduce_table3/../../development/models/sequential.py:1078\u001b[39m, in \u001b[36mSequential.convert_to_c\u001b[39m\u001b[34m(self, input_shape, var_name, src_dir, include_dir, for_arduino, test_input)\u001b[39m\n\u001b[32m   1074\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m layer_name, layer \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.names_layers():\n\u001b[32m   1076\u001b[39m     layers_def += \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m    &\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m,\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m-> \u001b[39m\u001b[32m1078\u001b[39m     layer_header, layer_def, layer_param_def = \u001b[43mlayer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconvert_to_c\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlayer_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minput_shape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfor_arduino\u001b[49m\u001b[43m=\u001b[49m\u001b[43mfor_arduino\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1079\u001b[39m     layers_header += layer_header\n\u001b[32m   1081\u001b[39m     param_definition_file += layer_param_def\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/EmbeddedAI/deep-microcompression/experiments/reproduce_table3/../../development/layers/identity.py:88\u001b[39m, in \u001b[36mIdentity.convert_to_c\u001b[39m\u001b[34m(self, var_name, input_shape, for_arduino)\u001b[39m\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mconvert_to_c\u001b[39m(\u001b[38;5;28mself\u001b[39m, var_name, input_shape, for_arduino=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mNotImplementedError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mThis is not implement because it should have been fused before deployment.\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[31mNotImplementedError\u001b[39m: This is not implement because it should have been fused before deployment."
     ]
    }
   ],
   "source": [
    "save_tflite_to_board(tflite_float_model, PROJECT_BASE_DIR)\n",
    "save_dmc_to_board(dmc_float_model, PROJECT_BASE_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
